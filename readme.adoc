= Knative + Kafka 

== Prerequisites

Assumes you have used try.openshift.com to create an OCP 4.x cluster.  Here is a quick video that illustrates that process.

https://youtu.be/sK2SWH_m01Y

CLI tools used:

* oc
* jq
* podman (or kafkacat)
* siege
* watch

If you don't have kafkacat, add alias: `alias kafkacat='podman run --rm -it edenhill/kafkacat:1.5.0'`

== Installation

Using the OCP 4.x Administration Console - find OperatorHub and install

* Serverless Operator
* Knative Eventing Operator
* Knative Kafka Operator
* AMQ Streams

image::images/operatorhub_ui.png[OperatorHub inside of OpenShift Console]

image::images/installed_operators.png[Installed Operators]

You can check on your installed operators and their versions: 

----
$ oc get csv
NAME                                        DISPLAY                          VERSION              REPLACES                            PHASE
amqstreams.v1.3.0                           AMQ Streams                      1.3.0                amqstreams.v1.2.0                   Succeeded
elasticsearch-operator.4.3.1-202002032140   Elasticsearch Operator           4.3.1-202002032140                                       Succeeded
jaeger-operator.v1.13.1                     Jaeger Operator                  1.13.1                                                   Succeeded
kiali-operator.v1.0.9                       Kiali Operator                   1.0.9                kiali-operator.v1.0.8               Succeeded
knative-eventing-operator.v0.12.0           Knative Eventing Operator        0.12.0               knative-eventing-operator.v0.11.0   Succeeded
knative-kafka-operator.v0.12.1              Knative Apache Kafka Operator    0.12.1               knative-kafka-operator.v0.11.2      Succeeded
serverless-operator.v1.4.1                  OpenShift Serverless Operator    1.4.1                serverless-operator.v1.4.0          Succeeded
servicemeshoperator.v1.0.7                  Red Hat OpenShift Service Mesh   1.0.7                servicemeshoperator.v1.0.6          Succeeded
----

You also can install those operators via commandline:

[source,bash]
----
cat <<EOF | oc apply -f -
# file: kube/install/0-rh-operators.yaml
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: serverless-operator
  namespace: openshift-operators
spec:
  channel: techpreview
  name: serverless-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: amq-streams
  namespace: openshift-operators
spec:
  channel: stable
  name: amq-streams
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
----

Due to OLM bug install community operators separately:

[source,bash]
----
cat <<EOF | oc apply -f -
# file: kube/install/1-community-operators.yaml
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: knative-eventing-operator
  namespace: openshift-operators
spec:
  channel: alpha
  name: knative-eventing-operator
  source: community-operators
  sourceNamespace: openshift-marketplace
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: knative-kafka-operator
  namespace: openshift-operators
spec:
  channel: alpha
  name: knative-kafka-operator
  source: community-operators
  sourceNamespace: openshift-marketplace
EOF
----


=== Install Knative Serving & Eventing

[source,bash]
----
cat <<EOF | oc apply -f -
# file: kube/install/2-knative-cr.yaml
---
kind: Namespace
apiVersion: v1
metadata:
  name: knative-serving
spec: {}
---
apiVersion: operator.knative.dev/v1alpha1
kind: KnativeServing
metadata:
  name: knative-serving
  namespace: knative-serving
spec: {}
---
apiVersion: eventing.knative.dev/v1alpha1
kind: KnativeEventing
metadata:
  name: knative-eventing
  namespace: knative-eventing
spec: {}
EOF
----

=== Namespace/Project Setup
[source,bash]
----
oc create namespace demo

# make it "sticky"
oc config set-context --current --namespace=demo

# check that it is set
oc config current-context

# or use "oc" to see what the "sticky" namespace is
oc project
----

== Create test resources

=== Create Kafka cluster
[source,bash]
----
cat <<EOF | oc apply -f -
# file: kube/demo/1-kafka.yaml
apiVersion: kafka.strimzi.io/v1beta1
kind: Kafka
metadata:
  name: my-cluster
  namespace: demo
spec:
  kafka:
    replicas: 3
    listeners:
      plain: {}
      tls: {}
      external:
        type: loadbalancer
        tls: false  
    config:
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
      log.message.format.version: '2.3'
    storage:
      type: jbod
      volumes:
      - id: 0
        type: persistent-claim
        size: 100Gi
        deleteClaim: true
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 100Gi
      deleteClaim: true
  entityOperator:
    topicOperator: {}
    userOperator: {}
EOF
----


=== Configure the Knative Eventing Kafka

Note: this only needs to be done one time
[source,bash]
----
cat <<EOF | oc apply -f -
# file: kube/demo/2-kn-eventing-kafka.yaml
---
apiVersion: eventing.knative.dev/v1alpha1
kind: KnativeEventingKafka
metadata:
  name: knative-eventing-kafka
  namespace: knative-eventing
spec:
  bootstrapServers: my-cluster-kafka-external-bootstrap.demo:9094
  setAsDefaultChannelProvisioner: false

EOF
----


Verify the KnativeEventingKafka took affect

[source,bash]
----
oc get crds | grep kafkasource
kafkasources.sources.eventing.knative.dev                   2019-09-21T14:23:14Z
----

and

[source,bash]
----
oc get pods -n knative-eventing

NAME                                            READY   STATUS              RESTARTS   AGE
eventing-controller-758d785bf7-wzq7v            1/1     Running             0          18m
eventing-webhook-7ff46cd45f-5tz9z               1/1     Running             0          18m
imc-controller-75d7f598df-b48bc                 1/1     Running             0          17m
imc-dispatcher-77f565585c-xb5c8                 1/1     Running             0          17m
in-memory-channel-controller-6b4967d97b-mlrdk   1/1     Running             0          18m
in-memory-channel-dispatcher-8bbcd4f9-t2gbj     1/1     Running             0          17m
kafka-ch-controller-5f55f4c58-9dm5j             0/1     ContainerCreating   0          11s
kafka-ch-dispatcher-5655cc4c9f-xbhv7            0/1     ContainerCreating   0          10s
kafka-channel-controller-578d46d7bd-fz6nf       0/1     ContainerCreating   0          19s
kafka-channel-dispatcher-b49d4bc54-dhbp9        0/1     ContainerCreating   0          17s
kafka-controller-manager-0                      0/1     ContainerCreating   0          20s
kafka-webhook-7c96f59b7f-9hzd5                  0/1     ContainerCreating   0          10s
sources-controller-788874d5fc-vl5mb             1/1     Running             0          18m
----


=== Create kafka topic

[source,bash]
----
cat <<EOF | oc apply -f -
# file: kube/demo/3-kafka-topic.yaml
---
apiVersion: kafka.strimzi.io/v1alpha1
kind: KafkaTopic
metadata:
  name: my-topic
  labels:
    strimzi.io/cluster: my-cluster
spec:
  partitions: 100
  replicas: 1
EOF
----


Test to see if the topic was created correctly

[source,bash]
----
oc exec -it -n demo -c kafka my-cluster-kafka-0 /bin/bash

bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic my-topic
----


OR

[source,bash]
----
oc exec -it -c kafka my-cluster-kafka-0 -- bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic my-topic

OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
Topic:my-topic	PartitionCount:100	ReplicationFactor:1	Configs:message.format.version=2.3-IV1
	Topic: my-topic	Partition: 0	Leader: 2	Replicas: 2	Isr: 2
	Topic: my-topic	Partition: 1	Leader: 0	Replicas: 0	Isr: 0
	Topic: my-topic	Partition: 2	Leader: 1	Replicas: 1	Isr: 1
	Topic: my-topic	Partition: 3	Leader: 2	Replicas: 2	Isr: 2
	Topic: my-topic	Partition: 4	Leader: 0	Replicas: 0	Isr: 0
	Topic: my-topic	Partition: 5	Leader: 1	Replicas: 1	Isr: 1
	Topic: my-topic	Partition: 6	Leader: 2	Replicas: 2	Isr: 2
.
.
.
----


== Test connectivity to the kafka my-topic

[source,bash]
----
# AWS
export BOOTSTRAP_IP=$(oc get services my-cluster-kafka-external-bootstrap -ojson | jq -r '.status.loadBalancer.ingress[].hostname')

#Azure
export BOOTSTRAP_IP=$(oc get services my-cluster-kafka-external-bootstrap -ojson | jq -r '.status.loadBalancer.ingress[].ip')

export BOOTSTRAP_PORT=$(oc get services my-cluster-kafka-external-bootstrap -ojson | jq -r '.spec.ports[].port')

export BOOTSTRAP_URL=$BOOTSTRAP_IP:$BOOTSTRAP_PORT
----


Then use Kafkacat to produce/consume messages
[source,bash]
----
kafkacat -P -b $BOOTSTRAP_URL -t my-topic
one
two
three
----

ctrl-z to end

[source,bash]
----
kafkacat -C -b $BOOTSTRAP_URL -t my-topic
one
% Reached end of topic my-topic [35] at offset 1
two
% Reached end of topic my-topic [81] at offset 1
three
% Reached end of topic my-topic [32] at offset 1
----

ctrl-c to end 

== Deploy a Knative Service

This is your "sink" that receives events

[source,bash]
----
cat <<EOF | oc apply -f -
# file: kube/demo/4-ksvc-sink.yaml
---
apiVersion: serving.knative.dev/v1alpha1
kind: Service
metadata:
  name: myknativesink
spec:
  template:
    metadata:
      annotations:
        autoscaling.knative.dev/target: "1"
    spec:
      containers:
      - image: docker.io/burrsutter/myknativesink:1.0.1
        resources:
          requests: 
            memory: "50Mi" 
            cpu: "100m" 
          limits:
            memory: "50Mi"
            cpu: "100m"       
        livenessProbe:
          httpGet:
            path: /healthz
        readinessProbe:
          httpGet:
            path: /healthz
EOF
----


If your pod is stuck in PENDING, check your events

[source,bash]
----
oc get events --sort-by=.metadata.creationTimestamp
----

You likely need to add another worker node (OpenShift Console - Compute - MachineSets)

image::images/machinesets.png[Machinesets]

== Create the KafkaSource that connects my-topic to ksvc 
[source,bash]
----
cat <<EOF | oc apply -f -
# file: kube/demo/5-kafkasource.yaml
apiVersion: sources.eventing.knative.dev/v1alpha1
kind: KafkaSource
metadata:
  name: mykafka-source
spec:
  consumerGroup: knative-group
  bootstrapServers: my-cluster-kafka-external-bootstrap.demo:9094
  topics: my-topic
  sink:
    apiVersion: serving.knative.dev/v1alpha1
    kind: Service
    name: myknativesink
EOF
----

----
# AWS
oc get services my-cluster-kafka-external-bootstrap -ojson | jq -r '.status.loadBalancer.ingress[].hostname'
#OR
#Azure
oc get services my-cluster-kafka-external-bootstrap -ojson | jq -r '.status.loadBalancer.ingress[].ip'

# and
oc get services my-cluster-kafka-external-bootstrap -ojson | jq -r '.spec.ports[].port'
----

You can monitor the logs of mmyknativesink-source to see if it has connectivity issues

----
stern mykafka-source
----

== Test

Now push some messages in, must be in JSON format 

----

kafkacat -P -b $BOOTSTRAP_URL -t my-topic
{"hello":"world"}
----

and you should see some logging output

[source,bash]
----
oc logs -l serving.knative.dev/configuration=myknativesink -c user-container
# or
kail -l serving.knative.dev/configuration=myknativesink -c user-container
# or
stern myknativesink -c user-container
----

----
myknativesink-h6l7x-deployment-54d58c84c5-q9sm5 user-container EVENT: {"hello":"world"}
----

image::images/hello_world_1.png[Waiting]


image::images/hello_world_2.png[Sink pod is up]


image::images/goodbye_world.png[one more message]


== Scaling beyond 1 Pod

Kafka-Producer is a simple little application that drives in 1, 10 or 100 messages as fast as it can.

Deploy kafka-producer

----
oc apply -f kube/demo/6-kafka-producer.yaml
----


Then drive some load
----
PRODUCER_URL="$(oc get route kafka-producer -ojson | jq -r '.status.ingress[].host')"
http $PRODUCER_URL/1
----

Watch the Developer Topology view

image::images/developer_topology.png[Developer View]

image::images/developer_topology_during_auto_scale.png[Developer View]

image::images/iterm_during_100.png[Terminal View]

== Clean up

[source,bash]
----
oc delete route kafka-producer
oc delete service kafka-producer
oc delete deployment kafka-producer
oc delete kafkasource mykafka-source
oc delete ksvc myknativesink
oc delete KafkaTopic my-topic
oc delete KnativeEventingKafka knative-eventing-kafka -n knative-eventing
oc delete kafka my-cluster
oc delete namespace demo
----

